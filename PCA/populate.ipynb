{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1d3e8e",
   "metadata": {},
   "source": [
    "# General PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd81a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db89fd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:1056: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:1080: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yearmon' column missing in combined_pca, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:1056: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:1080: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yearmon' column missing in imf_pca_top4, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing INFORM: at least one array or dtype is required\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:1056: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:1080: RuntimeWarning: invalid value encountered in cast\n",
      "  if (arr.astype(int) == arr).all():\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing VIEWS: at least one array or dtype is required\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
      "C:\\Users\\ishita banerjee\\AppData\\Local\\Temp\\ipykernel_27368\\2487652351.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary saved to 'PCA_Summary_Report.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get list of source files from the folder\n",
    "source_folder = \"Data_Sources\"\n",
    "csv_files = [f for f in os.listdir(source_folder) if f.endswith(\".csv\")]\n",
    "sources = [os.path.splitext(f)[0] for f in csv_files]\n",
    "\n",
    "# Summary list\n",
    "summary = []\n",
    "\n",
    "# Process each file\n",
    "for source in sources:\n",
    "    file_path = os.path.join(source_folder, f\"{source}.csv\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path).convert_dtypes()\n",
    "\n",
    "        # Preprocessing\n",
    "        if 'yearmon' not in df.columns:\n",
    "            print(f\"'yearmon' column missing in {source}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')  # KEEP datetime64 format\n",
    "        df = df.drop(columns=['yearmon'], errors='ignore')\n",
    "        df = df.set_index('date').sort_index()\n",
    "\n",
    "        # Track total columns before cleanup\n",
    "        data_columns = df.shape[1]\n",
    "\n",
    "        # Handle missing data\n",
    "        # Step 1: Create missing summary\n",
    "        total_rows = len(df)\n",
    "        missing_count = df.isnull().sum()\n",
    "        missing_percent = (missing_count / total_rows * 100).round(2)\n",
    "\n",
    "        missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_count,\n",
    "        'Missing Percentage (%)': missing_percent\n",
    "        }).sort_values(by='Missing Percentage (%)', ascending=False)\n",
    "\n",
    "        # Step 2: Drop columns with >70% overall missing AND >20% missing in last 5 years of the data\n",
    "        for col in missing_df.index:\n",
    "            if missing_percent[col] > 70:\n",
    "                if 'date' in df.columns and pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "                    max_date = df['date'].max()\n",
    "                    five_years_ago = max_date - pd.DateOffset(years=5)\n",
    "                    recent_df = df[df['date'] >= five_years_ago]\n",
    "                    recent_missing = recent_df[col].isnull().sum()\n",
    "                    recent_total = recent_df[col].shape[0]\n",
    "                    if recent_total == 0 or (recent_missing / recent_total * 100) > 20:\n",
    "                        df = df.drop(columns=[col])\n",
    "                    else:\n",
    "                        df = df.drop(columns=[col])  # Drop if no date context\n",
    "\n",
    "# Step 3: Extract clean numeric data\n",
    "        numeric_df = df.select_dtypes(include=['number'])\n",
    "        numeric_df = numeric_df.replace([np.inf, -np.inf], np.nan)\n",
    "        numeric_df = numeric_df.dropna(axis=1, how='all')  # Drop all-NaN columns\n",
    "        numeric_df = numeric_df.dropna(axis=0, how='any')  # Drop rows with any NaN\n",
    "\n",
    "\n",
    "        # Get numeric features\n",
    "        numeric_df = df.select_dtypes(include='number').dropna()\n",
    "        no_of_features = numeric_df.shape[1]\n",
    "\n",
    "        if no_of_features == 0:\n",
    "            print(f\"No numeric features after cleaning in {source}, skipping.\")\n",
    "            continue\n",
    "        # Replace inf/-inf with NaN     \n",
    "        numeric_df = numeric_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Drop columns where all values are NaN\n",
    "        numeric_df = numeric_df.dropna(axis=1, how='all')\n",
    "\n",
    "        # Drop rows with any remaining NaNs\n",
    "        numeric_df = numeric_df.dropna(axis=0, how='any')\n",
    "\n",
    "        # Perform PCA\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(numeric_df)\n",
    "\n",
    "        pca = PCA()\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "        # Count components with ≥ 0.05 explained variance (max 10)\n",
    "        significant_components = np.sum(explained_variance >= 0.05)\n",
    "        num_to_retain = min(significant_components, 10)\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "        total_explained_variance = (\n",
    "            round(cumulative_variance[num_to_retain - 1], 4) if num_to_retain > 0 else 0.0\n",
    "        )\n",
    "\n",
    "        # Append to summary\n",
    "        summary.append({\n",
    "            \"Source\": source,\n",
    "            \"Data Columns\": data_columns,\n",
    "            \"No of Features\": no_of_features,\n",
    "            \"No of Principal Components\": num_to_retain,\n",
    "            \"Total Explained Variance\": total_explained_variance\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save final summary\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_excel(\"PCA_Summary_Report.xlsx\", index=False)\n",
    "print(\"✅ Summary saved to 'PCA_Summary_Report.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3708576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ ACAPS: Retaining 2 PCs | Variance: 1.0\n",
      "✅ Saved: ACAPS_PCA_Output.xlsx\n",
      "ℹ️ ACLED: Retaining 10 PCs | Variance: 0.7057\n",
      "✅ Saved: ACLED_PCA_Output.xlsx\n",
      "ℹ️ BTI: Retaining 9 PCs | Variance: 1.0\n",
      "✅ Saved: BTI_PCA_Output.xlsx\n",
      "ℹ️ combined_pca: Retaining 9 PCs | Variance: 1.0\n",
      "✅ Saved: combined_pca_PCA_Output.xlsx\n",
      "ℹ️ CONFLICTFORECAST: Retaining 10 PCs | Variance: 0.541\n",
      "✅ Saved: CONFLICTFORECAST_PCA_Output.xlsx\n",
      "ℹ️ CPIA: Retaining 2 PCs | Variance: 1.0\n",
      "✅ Saved: CPIA_PCA_Output.xlsx\n",
      "ℹ️ CRISIS24: Retaining 3 PCs | Variance: 1.0\n",
      "✅ Saved: CRISIS24_PCA_Output.xlsx\n",
      "ℹ️ CRM: Retaining 4 PCs | Variance: 1.0\n",
      "✅ Saved: CRM_PCA_Output.xlsx\n",
      "ℹ️ CW: Retaining 4 PCs | Variance: 1.0\n",
      "✅ Saved: CW_PCA_Output.xlsx\n",
      "ℹ️ EIU: Retaining 4 PCs | Variance: 1.0\n",
      "✅ Saved: EIU_PCA_Output.xlsx\n",
      "ℹ️ EMDAT: Retaining 7 PCs | Variance: 1.0\n",
      "✅ Saved: EMDAT_PCA_Output.xlsx\n",
      "ℹ️ EPR: Retaining 9 PCs | Variance: 1.0\n",
      "✅ Saved: EPR_PCA_Output.xlsx\n",
      "ℹ️ FEWS: Retaining 10 PCs | Variance: 0.9714\n",
      "✅ Saved: FEWS_PCA_Output.xlsx\n",
      "ℹ️ FSI: Retaining 2 PCs | Variance: 1.0\n",
      "✅ Saved: FSI_PCA_Output.xlsx\n",
      "ℹ️ GDELT: Retaining 10 PCs | Variance: 0.9492\n",
      "✅ Saved: GDELT_PCA_Output.xlsx\n",
      "ℹ️ GIC: Retaining 7 PCs | Variance: 1.0\n",
      "✅ Saved: GIC_PCA_Output.xlsx\n",
      "ℹ️ IDMC: Retaining 5 PCs | Variance: 1.0\n",
      "✅ Saved: IDMC_PCA_Output.xlsx\n",
      "ℹ️ IFES: Retaining 8 PCs | Variance: 1.0\n",
      "✅ Saved: IFES_PCA_Output.xlsx\n",
      "ℹ️ IMF: Retaining 10 PCs | Variance: 0.8707\n",
      "✅ Saved: IMF_PCA_Output.xlsx\n",
      "ℹ️ imf_pca_top4: Retaining 4 PCs | Variance: 1.0\n",
      "✅ Saved: imf_pca_top4_PCA_Output.xlsx\n",
      "⚠️ All numeric features are constant in INFORM, skipping.\n",
      "ℹ️ INFORMSEVERITY: Retaining 10 PCs | Variance: 0.9999\n",
      "✅ Saved: INFORMSEVERITY_PCA_Output.xlsx\n",
      "ℹ️ NEG: Retaining 10 PCs | Variance: 0.9894\n",
      "✅ Saved: NEG_PCA_Output.xlsx\n",
      "ℹ️ POLECAT: Retaining 10 PCs | Variance: 0.8256\n",
      "✅ Saved: POLECAT_PCA_Output.xlsx\n",
      "ℹ️ POS: Retaining 10 PCs | Variance: 1.0\n",
      "✅ Saved: POS_PCA_Output.xlsx\n",
      "ℹ️ REIGN: Retaining 5 PCs | Variance: 1.0\n",
      "✅ Saved: REIGN_PCA_Output.xlsx\n",
      "ℹ️ SPEED: Retaining 6 PCs | Variance: 1.0\n",
      "✅ Saved: SPEED_PCA_Output.xlsx\n",
      "ℹ️ SPEI: Retaining 10 PCs | Variance: 0.9722\n",
      "✅ Saved: SPEI_PCA_Output.xlsx\n",
      "ℹ️ UCDP: Retaining 7 PCs | Variance: 1.0\n",
      "✅ Saved: UCDP_PCA_Output.xlsx\n",
      "ℹ️ UNDP: Retaining 2 PCs | Variance: 1.0\n",
      "✅ Saved: UNDP_PCA_Output.xlsx\n",
      "ℹ️ VDEM: Retaining 6 PCs | Variance: 1.0\n",
      "✅ Saved: VDEM_PCA_Output.xlsx\n",
      "⚠️ All numeric features are constant in VIEWS, skipping.\n",
      "ℹ️ WBG: Retaining 10 PCs | Variance: 0.9857\n",
      "✅ Saved: WBG_PCA_Output.xlsx\n",
      "ℹ️ WDI: Retaining 2 PCs | Variance: 1.0\n",
      "✅ Saved: WDI_PCA_Output.xlsx\n",
      "ℹ️ WGI: Retaining 7 PCs | Variance: 1.0\n",
      "✅ Saved: WGI_PCA_Output.xlsx\n"
     ]
    }
   ],
   "source": [
    "all_pcs = []\n",
    "\n",
    "for source in sources:\n",
    "    file_path = os.path.join(source_folder, f\"{source}.csv\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path).convert_dtypes()\n",
    "\n",
    "        # Convert yearmon to datetime\n",
    "        if 'yearmon' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['yearmon'], errors='coerce')\n",
    "\n",
    "        # Harmonize region_code to regioncode\n",
    "        if 'region_code' in df.columns and 'regioncode' not in df.columns:\n",
    "            df['regioncode'] = df['region_code']\n",
    "\n",
    "        # Select numeric features\n",
    "        numeric_df = df.select_dtypes(include='number').replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        if numeric_df.shape[1] == 0:\n",
    "            print(f\"⚠️ No numeric features in {source}, skipping.\")\n",
    "            continue\n",
    "        if numeric_df.nunique().max() <= 1:\n",
    "            print(f\"⚠️ All numeric features are constant in {source}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        aligned_rows = numeric_df.index\n",
    "\n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(numeric_df)\n",
    "\n",
    "        # PCA\n",
    "        pca = PCA()\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "        # Retain up to 10 PCs or until cumulative variance reaches 1.0\n",
    "        num_to_retain = np.argmax(cumulative_variance >= 1.0) + 1 if np.any(cumulative_variance >= 1.0) else len(cumulative_variance)\n",
    "        num_to_retain = min(num_to_retain, 10)\n",
    "\n",
    "        print(f\"ℹ️ {source}: Retaining {num_to_retain} PCs | Variance: {round(cumulative_variance[num_to_retain-1], 4)}\")\n",
    "\n",
    "        # Create PCA DataFrame with prefixed column names\n",
    "        pc_cols = [f\"{source}_PC{i+1}\" for i in range(num_to_retain)]\n",
    "        pc_df = pd.DataFrame(X_pca[:, :num_to_retain], columns=pc_cols, index=aligned_rows)\n",
    "\n",
    "        # Extract meta columns\n",
    "        meta_cols = ['iso3', 'regioncode', 'date']\n",
    "        meta_df = df.loc[aligned_rows, [col for col in meta_cols if col in df.columns]].reset_index(drop=True)\n",
    "\n",
    "        # Combine and save\n",
    "        combined_df = pd.concat([meta_df, pc_df.reset_index(drop=True)], axis=1)\n",
    "        all_pcs.append(combined_df)\n",
    "\n",
    "        # Save each file separately\n",
    "        output_file = f\"{source}_PCA_Output.xlsx\"\n",
    "        combined_df.to_excel(output_file, index=False)\n",
    "        print(f\"✅ Saved: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {source}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39154539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58713, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9945516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58713, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff9c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Not enough numeric features for correlation in ACAPS. Skipping.\n",
      "✅ Saved: ACLED_correlations_filtered.xlsx\n",
      "✅ Saved: BTI_correlations_filtered.xlsx\n",
      "⚠️ No strong correlations found in combined_pca.\n",
      "✅ Saved: CONFLICTFORECAST_correlations_filtered.xlsx\n",
      "⚠️ Not enough numeric features for correlation in CPIA. Skipping.\n",
      "⚠️ Not enough numeric features for correlation in CRISIS24. Skipping.\n",
      "⚠️ Not enough numeric features for correlation in CRM. Skipping.\n",
      "⚠️ Not enough numeric features for correlation in CW. Skipping.\n",
      "✅ Saved: EIU_correlations_filtered.xlsx\n",
      "✅ Saved: EMDAT_correlations_filtered.xlsx\n",
      "⚠️ No strong correlations found in EPR.\n",
      "⚠️ Not enough numeric features for correlation in FEWS. Skipping.\n",
      "⚠️ No strong correlations found in FSI.\n",
      "✅ Saved: GDELT_correlations_filtered.xlsx\n",
      "✅ Saved: GIC_correlations_filtered.xlsx\n",
      "✅ Saved: IDMC_correlations_filtered.xlsx\n",
      "⚠️ Not enough numeric features for correlation in IFES. Skipping.\n",
      "✅ Saved: IMF_correlations_filtered.xlsx\n",
      "⚠️ No strong correlations found in imf_pca_top4.\n",
      "⚠️ Not enough numeric features for correlation in INFORM. Skipping.\n",
      "⚠️ Not enough numeric features for correlation in INFORMSEVERITY. Skipping.\n",
      "⚠️ Not enough numeric features for correlation in NEG. Skipping.\n",
      "✅ Saved: POLECAT_correlations_filtered.xlsx\n",
      "✅ Saved: POS_correlations_filtered.xlsx\n",
      "⚠️ No strong correlations found in REIGN.\n",
      "⚠️ Not enough numeric features for correlation in SPEED. Skipping.\n",
      "✅ Saved: SPEI_correlations_filtered.xlsx\n",
      "✅ Saved: UCDP_correlations_filtered.xlsx\n",
      "⚠️ No strong correlations found in UNDP.\n",
      "✅ Saved: VDEM_correlations_filtered.xlsx\n",
      "✅ Saved: VIEWS_correlations_filtered.xlsx\n",
      "✅ Saved: WBG_correlations_filtered.xlsx\n",
      "⚠️ No strong correlations found in WDI.\n",
      "✅ Saved: WGI_correlations_filtered.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Folder path\n",
    "source_folder = \"Data_Sources\"\n",
    "csv_files = [f for f in os.listdir(source_folder) if f.endswith(\".csv\")]\n",
    "sources = [os.path.splitext(f)[0] for f in csv_files]\n",
    "\n",
    "for source in sources:\n",
    "    file_path = os.path.join(source_folder, f\"{source}.csv\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path).convert_dtypes()\n",
    "\n",
    "        # Parse 'yearmon' to 'date' if available\n",
    "        if 'yearmon' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['yearmon'], format=\"%b %Y\", errors='coerce')\n",
    "\n",
    "        # Step 1: Missing summary\n",
    "        total_rows = len(df)\n",
    "        missing_count = df.isnull().sum()\n",
    "        missing_percent = (missing_count / total_rows * 100).round(2)\n",
    "\n",
    "        # Step 2: Drop columns with >70% overall AND >20% missing in last 5 years\n",
    "        for col in missing_percent.index:\n",
    "            if missing_percent[col] > 70:\n",
    "                if 'date' in df.columns and pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "                    max_date = df['date'].max()\n",
    "                    five_years_ago = max_date - pd.DateOffset(years=5)\n",
    "                    recent_df = df[df['date'] >= five_years_ago]\n",
    "                    recent_missing = recent_df[col].isnull().sum()\n",
    "                    recent_total = recent_df[col].shape[0]\n",
    "                    if recent_total == 0 or (recent_missing / recent_total * 100) > 20:\n",
    "                        df = df.drop(columns=[col])\n",
    "                else:\n",
    "                    df = df.drop(columns=[col])\n",
    "\n",
    "        # Step 3: Clean numeric data\n",
    "        numeric_df = df.select_dtypes(include=['number'])\n",
    "        numeric_df = numeric_df.replace([np.inf, -np.inf], np.nan)\n",
    "        numeric_df = numeric_df.dropna(axis=1, how='all')\n",
    "        numeric_df = numeric_df.dropna(axis=0, how='any')\n",
    "\n",
    "        if numeric_df.shape[1] < 2:\n",
    "            print(f\"⚠️ Not enough numeric features for correlation in {source}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Step 4: Correlation matrix\n",
    "        correlation_matrix_df = numeric_df.corr(method='pearson')\n",
    "\n",
    "        # Step 5: Filter strong correlations (abs > 0.5, no diagonal)\n",
    "        corr_matrix = correlation_matrix_df.copy()\n",
    "        mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        filtered_corr = corr_matrix.where(mask)\n",
    "\n",
    "        tidy_corr = (\n",
    "            filtered_corr.stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={'level_0': 'Variable 1', 'level_1': 'Variable 2', 0: 'Correlation'})\n",
    "        )\n",
    "        tidy_corr = tidy_corr[(tidy_corr['Correlation'] > 0.5) | (tidy_corr['Correlation'] < -0.5)]\n",
    "\n",
    "        if tidy_corr.empty:\n",
    "            print(f\"⚠️ No strong correlations found in {source}.\")\n",
    "            continue\n",
    "\n",
    "        # Add source column\n",
    "        tidy_corr.insert(0, 'Source', source)\n",
    "\n",
    "        # Save to Excel\n",
    "        output_file = f\"{source}_correlations_filtered.xlsx\"\n",
    "        tidy_corr.to_excel(output_file, index=False)\n",
    "        print(f\"✅ Saved: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {source}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
